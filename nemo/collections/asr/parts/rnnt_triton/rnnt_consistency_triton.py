# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import torch
import triton
import triton.language as tl


@triton.jit
def _kl_div_fwd_kernel(
    teacher_logits_ptr,
    student_logits_ptr,
    mask_ptr,
    max_source_len: int,
    max_target_len_plus_1: int,
    num_labels: int,  # vocab size (with blank)
    kl_loss_out_ptr,
    BLOCK_SIZE: tl.constexpr,
    USE_FP64: tl.constexpr,
    SYMMETRIC: tl.constexpr,
):
    """
    Forward kernel for KL divergence loss.

    When SYMMETRIC=False: computes KL(P||Q) = sum(P * (log P - log Q))
    When SYMMETRIC=True: computes 0.5 * (KL(P||Q) + KL(Q||P)) = 0.5 * sum((P - Q) * (log P - log Q))

    Calculations are performed in float32 or float64 based on USE_FP64.
    """
    batch_i = tl.program_id(axis=0).to(tl.int64)
    source_i = tl.program_id(axis=1).to(tl.int64)
    target_i = tl.program_id(axis=2).to(tl.int64)

    idx_no_vocab = (batch_i * max_source_len + source_i) * max_target_len_plus_1 + target_i
    mask_value = tl.load(mask_ptr + idx_no_vocab)
    if not mask_value:
        return

    compute_dtype = tl.float64 if USE_FP64 else tl.float32

    # calculate offset in [B, T, U+1, V] tensor
    idx_vocab_start = idx_no_vocab * num_labels
    teacher_logits_ptr += idx_vocab_start
    student_logits_ptr += idx_vocab_start
    col_offsets = tl.arange(0, BLOCK_SIZE)
    mask = col_offsets < num_labels
    teacher_logits = tl.load(teacher_logits_ptr + col_offsets, mask=mask, other=-float("inf")).to(compute_dtype)
    student_logits = tl.load(student_logits_ptr + col_offsets, mask=mask, other=-float("inf")).to(compute_dtype)

    # stable log softmax calculation for teacher
    teacher_logits_max = tl.max(teacher_logits, axis=0)
    teacher_logits_minus_max = teacher_logits - teacher_logits_max
    teacher_denominator = tl.log(tl.sum(tl.exp(teacher_logits_minus_max), axis=0))
    teacher_log_softmax = teacher_logits_minus_max - teacher_denominator
    teacher_softmax = tl.exp(teacher_log_softmax)

    # stable log softmax calculation for student
    student_logits_max = tl.max(student_logits, axis=0)
    student_logits_minus_max = student_logits - student_logits_max
    student_denominator = tl.log(tl.sum(tl.exp(student_logits_minus_max), axis=0))
    student_log_softmax = student_logits_minus_max - student_denominator

    if SYMMETRIC:
        # symmetric KL: 0.5 * sum((P - Q) * (log P - log Q))
        student_softmax = tl.exp(student_log_softmax)
        prob_diff = teacher_softmax - student_softmax
        log_prob_diff = teacher_log_softmax - student_log_softmax
        kl_per_vocab = prob_diff * log_prob_diff
        kl_loss_value = 0.5 * tl.sum(tl.where(mask, kl_per_vocab, 0.0), axis=0)
    else:
        # non-symmetric KL: sum(P * (log P - log Q))
        kl_per_vocab = teacher_softmax * (teacher_log_softmax - student_log_softmax)
        kl_loss_value = tl.sum(tl.where(mask, kl_per_vocab, 0.0), axis=0)

    tl.store(kl_loss_out_ptr + idx_no_vocab, kl_loss_value)


@triton.jit
def _kl_div_bwd_kernel(
    teacher_logits_ptr,
    student_logits_ptr,
    grad_kl_loss_ptr,  # upstream gradient [B, T, U+1]
    mask_ptr,
    max_source_len: int,
    max_target_len_plus_1: int,
    num_labels: int,  # vocab size (with blank)
    teacher_grad_out_ptr,  # only used when SYMMETRIC=True
    student_grad_out_ptr,
    BLOCK_SIZE: tl.constexpr,
    USE_FP64: tl.constexpr,
    SYMMETRIC: tl.constexpr,
):
    """
    Backward kernel for KL divergence loss.

    When SYMMETRIC=False: computes student_grad = upstream * (Q - P), teacher receives no gradient
    When SYMMETRIC=True: computes student_grad = 0.5 * upstream * (Q - P), teacher_grad = -student_grad

    We recalculate log-softmax in backward instead of storing it in memory.
    Calculations are performed in float32 or float64 based on USE_FP64.
    """
    batch_i = tl.program_id(axis=0).to(tl.int64)
    source_i = tl.program_id(axis=1).to(tl.int64)
    target_i = tl.program_id(axis=2).to(tl.int64)

    idx_no_vocab = (batch_i * max_source_len + source_i) * max_target_len_plus_1 + target_i
    mask_value = tl.load(mask_ptr + idx_no_vocab)
    if not mask_value:
        return

    compute_dtype = tl.float64 if USE_FP64 else tl.float32

    # calculate offset in [B, T, U+1, V] tensor
    idx_vocab_start = idx_no_vocab * num_labels
    teacher_logits_ptr += idx_vocab_start
    student_logits_ptr += idx_vocab_start
    col_offsets = tl.arange(0, BLOCK_SIZE)
    mask = col_offsets < num_labels
    teacher_logits = tl.load(teacher_logits_ptr + col_offsets, mask=mask, other=-float("inf")).to(compute_dtype)
    student_logits = tl.load(student_logits_ptr + col_offsets, mask=mask, other=-float("inf")).to(compute_dtype)

    # stable log softmax calculation for teacher
    teacher_logits_max = tl.max(teacher_logits, axis=0)
    teacher_logits_minus_max = teacher_logits - teacher_logits_max
    teacher_denominator = tl.log(tl.sum(tl.exp(teacher_logits_minus_max), axis=0))
    teacher_softmax = tl.exp(teacher_logits_minus_max - teacher_denominator)

    # stable log softmax calculation for student
    student_logits_max = tl.max(student_logits, axis=0)
    student_logits_minus_max = student_logits - student_logits_max
    student_denominator = tl.log(tl.sum(tl.exp(student_logits_minus_max), axis=0))
    student_softmax = tl.exp(student_logits_minus_max - student_denominator)

    # load upstream gradient
    upstream_grad = tl.load(grad_kl_loss_ptr + idx_no_vocab).to(compute_dtype)

    # compute gradient: (Q - P) where Q = student_softmax, P = teacher_softmax
    prob_diff = student_softmax - teacher_softmax

    if SYMMETRIC:
        # symmetric: student_grad = 0.5 * upstream * (Q - P), teacher_grad = -student_grad
        student_grad = 0.5 * upstream_grad * prob_diff
        teacher_grad = -student_grad
        # store teacher gradient
        teacher_grad_out_ptr += idx_vocab_start
        tl.store(teacher_grad_out_ptr + col_offsets, teacher_grad, mask=mask)
    else:
        # non-symmetric: student_grad = upstream * (Q - P), no teacher gradient
        student_grad = upstream_grad * prob_diff

    # store student gradient
    student_grad_out_ptr += idx_vocab_start
    tl.store(student_grad_out_ptr + col_offsets, student_grad, mask=mask)


class FusedKLDivTriton(torch.autograd.Function):
    """
    Function to calculate KL divergence for RNN-T, supporting torch.autograd.

    When symmetric=False: computes KL(P||Q), only student receives gradients (teacher is detached)
    When symmetric=True: computes 0.5 * (KL(P||Q) + KL(Q||P)), both teacher and student receive gradients
    """

    @staticmethod
    def forward(
        ctx,
        teacher_logits: torch.Tensor,
        student_logits: torch.Tensor,
        mask: torch.Tensor,
        symmetric: bool = False,
    ):
        """
        Args:
            ctx: ctx object for storing the context
            teacher_logits: Joint tensor of size [B, T, U+1, D]
            student_logits: Joint tensor of size [B, T, U+1, D]
            mask: mask tensor [B, T, U+1]
            symmetric: if True, compute symmetric KL divergence

        Returns:
            loss of size [B, T, U+1]
        """
        assert teacher_logits.is_contiguous()
        assert student_logits.is_contiguous()
        assert teacher_logits.shape == student_logits.shape
        use_fp64 = teacher_logits.dtype == torch.float64

        kl_loss = teacher_logits.new_zeros(teacher_logits.shape[:-1])
        _kl_div_fwd_kernel[(teacher_logits.shape[0], teacher_logits.shape[1], teacher_logits.shape[2])](
            teacher_logits_ptr=teacher_logits,
            student_logits_ptr=student_logits,
            mask_ptr=mask,
            max_source_len=teacher_logits.shape[1],
            max_target_len_plus_1=teacher_logits.shape[2],
            num_labels=teacher_logits.shape[3],
            kl_loss_out_ptr=kl_loss,
            BLOCK_SIZE=triton.next_power_of_2(teacher_logits.shape[-1]),
            USE_FP64=use_fp64,
            SYMMETRIC=symmetric,
        )

        ctx.save_for_backward(teacher_logits, student_logits, mask)
        ctx.use_fp64 = use_fp64
        ctx.symmetric = symmetric
        return kl_loss

    @staticmethod
    def backward(ctx, grad_kl_loss):
        """
        Backward calculation for KL divergence loss.

        Args:
            ctx: ctx object for storing the context
            grad_kl_loss: upstream gradient [B, T, U+1]

        Returns:
            Gradients for teacher_logits (None if not symmetric), student_logits, mask (None), symmetric (None)
        """
        (teacher_logits, student_logits, mask) = ctx.saved_tensors
        use_fp64 = ctx.use_fp64
        symmetric = ctx.symmetric

        student_grad_logits = torch.zeros_like(student_logits)
        teacher_grad_logits = torch.zeros_like(teacher_logits) if symmetric else student_grad_logits  # dummy if not symmetric
        grad_kl_loss = grad_kl_loss.contiguous()

        _kl_div_bwd_kernel[(teacher_logits.shape[0], teacher_logits.shape[1], teacher_logits.shape[2])](
            teacher_logits_ptr=teacher_logits,
            student_logits_ptr=student_logits,
            grad_kl_loss_ptr=grad_kl_loss,
            mask_ptr=mask,
            max_source_len=teacher_logits.shape[1],
            max_target_len_plus_1=teacher_logits.shape[2],
            num_labels=teacher_logits.shape[3],
            teacher_grad_out_ptr=teacher_grad_logits,
            student_grad_out_ptr=student_grad_logits,
            BLOCK_SIZE=triton.next_power_of_2(teacher_logits.shape[-1]),
            USE_FP64=use_fp64,
            SYMMETRIC=symmetric,
        )

        if symmetric:
            return teacher_grad_logits, student_grad_logits, None, None
        else:
            return None, student_grad_logits, None, None


def kl_loss_triton(
    teacher_logits: torch.Tensor,
    student_logits: torch.Tensor,
    mask: torch.Tensor,
    symmetrical: bool = False,
) -> torch.Tensor:
    """
    Memory-efficient implementation of kl-div loss for RNN-T in Triton

    Args:
        teacher_logits: Joint tensor of size [B, T, U+1, D]
        student_logits: Joint tensor of size [B, T, U+1, D]
        mask: mask tensor [B, T, U+1]
        symmetrical: if loss is symmetrical

    Returns:
        tensor of size [B, T, U+1] with consistency loss
    """
    if symmetrical:
        return FusedKLDivTriton.apply(teacher_logits, student_logits, mask, True)
    else:
        # Non-symmetric: only student receives gradients, teacher is detached
        return FusedKLDivTriton.apply(teacher_logits.detach(), student_logits, mask, False)
